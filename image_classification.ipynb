{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 5:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1014, 1: 1014, 2: 952, 3: 1016, 4: 997, 5: 1025, 6: 980, 7: 977, 8: 1003, 9: 1022}\n",
      "First 20 Labels: [1, 8, 5, 1, 5, 7, 4, 3, 8, 2, 7, 2, 0, 1, 5, 9, 6, 2, 0, 8]\n",
      "\n",
      "Example of Image 50:\n",
      "Image - Min Value: 4 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 9 Name: truck\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6483ef9a20>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 5\n",
    "sample_id = 50\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x/255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Look into LabelBinarizer in the preprocessing module of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "encode_mapping = {key: np.eye(1, 10, key)[0] for key in range(0,10)}\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    one_hot_labels = []\n",
    "    for i in x:\n",
    "        one_hot_label = encode_mapping[i]\n",
    "        one_hot_labels.append(one_hot_label)\n",
    "    return np.array(one_hot_labels)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = (None,) + image_shape\n",
    "    x = tf.placeholder(tf.float32, shape=shape, name=\"x\")\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = (None, n_classes)\n",
    "    y = tf.placeholder(tf.float32, shape=shape, name=\"y\")\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob = tf.placeholder(tf.float32, shape=None, name=\"keep_prob\")\n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers.\n",
    "\n",
    "** Hint: **\n",
    "\n",
    "When unpacking values as an argument in Python, look into the [unpacking](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    w_shape = (conv_ksize[0], conv_ksize[1], x_tensor.get_shape().as_list()[-1], conv_num_outputs)\n",
    "    W = tf.Variable(tf.random_normal(w_shape))*np.sqrt(2.0/np.prod(w_shape[:-1]))\n",
    "    b = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "\n",
    "    conv = tf.nn.conv2d(x_tensor, W, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    conv = tf.nn.bias_add(conv, b)\n",
    "    \n",
    "    conv = tf.nn.relu(conv)\n",
    "    \n",
    "    conv = tf.nn.max_pool(conv, \\\n",
    "                          ksize=[1, pool_ksize[0], pool_ksize[1], 1], \\\n",
    "                          strides=[1, pool_strides[0], pool_strides[1], 1], \\\n",
    "                          padding='SAME')\n",
    "    \n",
    "    return conv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.reshape(x_tensor, [tf.shape(x_tensor)[0], np.prod(x_tensor.get_shape().as_list()[1:])])\n",
    "    return None\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    w_shape = (x_tensor.get_shape().as_list()[1], num_outputs)\n",
    "    W = tf.Variable(tf.random_normal(w_shape)*np.sqrt(2.0/x_tensor.get_shape().as_list()[1]))\n",
    "    b = tf.Variable(tf.zeros([num_outputs]))    \n",
    "    fc = tf.add(tf.matmul(x_tensor, W), b)\n",
    "    fc = tf.nn.relu(fc)\n",
    "\n",
    "    return fc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    w_shape = (x_tensor.get_shape().as_list()[1], num_outputs)\n",
    "    W = tf.Variable(tf.random_normal(w_shape))\n",
    "    b = tf.Variable(tf.zeros([num_outputs]))\n",
    "    \n",
    "    out = tf.add(tf.matmul(x_tensor, W), b)\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_out = conv2d_maxpool(x, conv_num_outputs=32, conv_ksize=(3, 3), \\\n",
    "                          conv_strides=(1, 1), pool_ksize=(2, 2), pool_strides=(2, 2))\n",
    "    conv_out = conv2d_maxpool(conv_out, conv_num_outputs=64, conv_ksize=(3, 3), \\\n",
    "                          conv_strides=(1, 1), pool_ksize=(2, 2), pool_strides=(2, 2))\n",
    "    conv_out = conv2d_maxpool(conv_out, conv_num_outputs=128, conv_ksize=(3, 3), \\\n",
    "                          conv_strides=(1, 1), pool_ksize=(2, 2), pool_strides=(2, 2))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat = flatten(conv_out)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc_out = fully_conn(flat, 512) \n",
    "    fc_out = tf.nn.dropout(fc_out, keep_prob)\n",
    "    fc_out = fully_conn(fc_out, 256) \n",
    "    fc_out = tf.nn.dropout(fc_out, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    final_out = output(fc_out, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return final_out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = sess.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    train_acc = session.run(accuracy, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    val_acc = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.})\n",
    "\n",
    "    print('Loss: {:.6f} Training Accuracy: {:.4f} Validation Accuracy: {:.4f}'.format(loss, train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 70\n",
    "batch_size = 4096\n",
    "keep_probability = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 18.148073 Training Accuracy: 0.1163 Validation Accuracy: 0.1070\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 4.762289 Training Accuracy: 0.1126 Validation Accuracy: 0.1248\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 3.326774 Training Accuracy: 0.1522 Validation Accuracy: 0.1316\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 2.958423 Training Accuracy: 0.1361 Validation Accuracy: 0.1434\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 2.360929 Training Accuracy: 0.2017 Validation Accuracy: 0.1754\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 2.111879 Training Accuracy: 0.2550 Validation Accuracy: 0.2052\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 2.087332 Training Accuracy: 0.2228 Validation Accuracy: 0.2074\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 2.014515 Training Accuracy: 0.2611 Validation Accuracy: 0.2254\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.960094 Training Accuracy: 0.3094 Validation Accuracy: 0.2510\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.948684 Training Accuracy: 0.3131 Validation Accuracy: 0.2814\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.961433 Training Accuracy: 0.3218 Validation Accuracy: 0.2714\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.953835 Training Accuracy: 0.3193 Validation Accuracy: 0.2854\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.939096 Training Accuracy: 0.3144 Validation Accuracy: 0.3008\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.913379 Training Accuracy: 0.3391 Validation Accuracy: 0.3034\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.866166 Training Accuracy: 0.3577 Validation Accuracy: 0.3146\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.833413 Training Accuracy: 0.3824 Validation Accuracy: 0.3154\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.785983 Training Accuracy: 0.3837 Validation Accuracy: 0.3308\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.757834 Training Accuracy: 0.3948 Validation Accuracy: 0.3360\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.722914 Training Accuracy: 0.4183 Validation Accuracy: 0.3442\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.689805 Training Accuracy: 0.4282 Validation Accuracy: 0.3564\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.660494 Training Accuracy: 0.4394 Validation Accuracy: 0.3694\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.643524 Training Accuracy: 0.4455 Validation Accuracy: 0.3700\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.619491 Training Accuracy: 0.4505 Validation Accuracy: 0.3796\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.588383 Training Accuracy: 0.4752 Validation Accuracy: 0.3786\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.562340 Training Accuracy: 0.5062 Validation Accuracy: 0.3822\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.531251 Training Accuracy: 0.5025 Validation Accuracy: 0.3890\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.495188 Training Accuracy: 0.5186 Validation Accuracy: 0.4036\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.469861 Training Accuracy: 0.5198 Validation Accuracy: 0.4030\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.444170 Training Accuracy: 0.5260 Validation Accuracy: 0.4078\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.418374 Training Accuracy: 0.5334 Validation Accuracy: 0.4132\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.377686 Training Accuracy: 0.5545 Validation Accuracy: 0.4222\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.351103 Training Accuracy: 0.5619 Validation Accuracy: 0.4222\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 1.325251 Training Accuracy: 0.5767 Validation Accuracy: 0.4236\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.298388 Training Accuracy: 0.5829 Validation Accuracy: 0.4314\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 1.275170 Training Accuracy: 0.6040 Validation Accuracy: 0.4328\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 1.242741 Training Accuracy: 0.6040 Validation Accuracy: 0.4410\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 1.217971 Training Accuracy: 0.6151 Validation Accuracy: 0.4380\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 1.182282 Training Accuracy: 0.6139 Validation Accuracy: 0.4442\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 1.153821 Training Accuracy: 0.6262 Validation Accuracy: 0.4528\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 1.129388 Training Accuracy: 0.6423 Validation Accuracy: 0.4542\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 1.110396 Training Accuracy: 0.6609 Validation Accuracy: 0.4488\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 1.079421 Training Accuracy: 0.6634 Validation Accuracy: 0.4576\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 1.061705 Training Accuracy: 0.6757 Validation Accuracy: 0.4638\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 1.036626 Training Accuracy: 0.6856 Validation Accuracy: 0.4592\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 1.012154 Training Accuracy: 0.7017 Validation Accuracy: 0.4652\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.993848 Training Accuracy: 0.7030 Validation Accuracy: 0.4652\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.973620 Training Accuracy: 0.7178 Validation Accuracy: 0.4742\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.940347 Training Accuracy: 0.7079 Validation Accuracy: 0.4716\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.920518 Training Accuracy: 0.7302 Validation Accuracy: 0.4722\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.903633 Training Accuracy: 0.7389 Validation Accuracy: 0.4750\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.880294 Training Accuracy: 0.7364 Validation Accuracy: 0.4752\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.858715 Training Accuracy: 0.7587 Validation Accuracy: 0.4882\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.840096 Training Accuracy: 0.7661 Validation Accuracy: 0.4802\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.820359 Training Accuracy: 0.7785 Validation Accuracy: 0.4888\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.792114 Training Accuracy: 0.7809 Validation Accuracy: 0.4852\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.783335 Training Accuracy: 0.7933 Validation Accuracy: 0.4884\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.762399 Training Accuracy: 0.7933 Validation Accuracy: 0.4870\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.751681 Training Accuracy: 0.8106 Validation Accuracy: 0.4870\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.721824 Training Accuracy: 0.8168 Validation Accuracy: 0.4940\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.700632 Training Accuracy: 0.8280 Validation Accuracy: 0.4944\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.694633 Training Accuracy: 0.8280 Validation Accuracy: 0.4968\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.678551 Training Accuracy: 0.8329 Validation Accuracy: 0.4972\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.661359 Training Accuracy: 0.8428 Validation Accuracy: 0.4984\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.643179 Training Accuracy: 0.8577 Validation Accuracy: 0.4950\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.629943 Training Accuracy: 0.8540 Validation Accuracy: 0.5034\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.617913 Training Accuracy: 0.8564 Validation Accuracy: 0.5048\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.592342 Training Accuracy: 0.8738 Validation Accuracy: 0.5036\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.581291 Training Accuracy: 0.8762 Validation Accuracy: 0.4996\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.569666 Training Accuracy: 0.8800 Validation Accuracy: 0.5060\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.561439 Training Accuracy: 0.8824 Validation Accuracy: 0.5064\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 29.027721 Training Accuracy: 0.1324 Validation Accuracy: 0.1324\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 12.377590 Training Accuracy: 0.1176 Validation Accuracy: 0.1172\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 3.717221 Training Accuracy: 0.1547 Validation Accuracy: 0.1356\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 4.597437 Training Accuracy: 0.0941 Validation Accuracy: 0.1064\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 2.942340 Training Accuracy: 0.1621 Validation Accuracy: 0.1620\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.813092 Training Accuracy: 0.1386 Validation Accuracy: 0.1268\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 2.241250 Training Accuracy: 0.1844 Validation Accuracy: 0.1884\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 2.158926 Training Accuracy: 0.2017 Validation Accuracy: 0.2146\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 2.103554 Training Accuracy: 0.2240 Validation Accuracy: 0.2154\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 2.096689 Training Accuracy: 0.2129 Validation Accuracy: 0.2346\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.087191 Training Accuracy: 0.2450 Validation Accuracy: 0.2420\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 2.011909 Training Accuracy: 0.2797 Validation Accuracy: 0.2600\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 2.012516 Training Accuracy: 0.2574 Validation Accuracy: 0.2698\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 1.948323 Training Accuracy: 0.3032 Validation Accuracy: 0.2726\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 1.995406 Training Accuracy: 0.2970 Validation Accuracy: 0.2784\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.958745 Training Accuracy: 0.3045 Validation Accuracy: 0.2958\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 1.922153 Training Accuracy: 0.3230 Validation Accuracy: 0.3066\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.901182 Training Accuracy: 0.3181 Validation Accuracy: 0.3150\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.843786 Training Accuracy: 0.3391 Validation Accuracy: 0.3250\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.894564 Training Accuracy: 0.3379 Validation Accuracy: 0.3278\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.846732 Training Accuracy: 0.3540 Validation Accuracy: 0.3284\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.815012 Training Accuracy: 0.3502 Validation Accuracy: 0.3402\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 1.785826 Training Accuracy: 0.3762 Validation Accuracy: 0.3562\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.754341 Training Accuracy: 0.3837 Validation Accuracy: 0.3642\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 1.798789 Training Accuracy: 0.3663 Validation Accuracy: 0.3726\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.772272 Training Accuracy: 0.3812 Validation Accuracy: 0.3792\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 1.722364 Training Accuracy: 0.3923 Validation Accuracy: 0.3832\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 1.702076 Training Accuracy: 0.4059 Validation Accuracy: 0.3872\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 1.674912 Training Accuracy: 0.3948 Validation Accuracy: 0.3836\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 1.711048 Training Accuracy: 0.4059 Validation Accuracy: 0.3936\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.682132 Training Accuracy: 0.4220 Validation Accuracy: 0.3986\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 1.633249 Training Accuracy: 0.4183 Validation Accuracy: 0.3984\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 1.624213 Training Accuracy: 0.4332 Validation Accuracy: 0.4068\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 1.596787 Training Accuracy: 0.4406 Validation Accuracy: 0.4144\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 1.641123 Training Accuracy: 0.4431 Validation Accuracy: 0.4194\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.613452 Training Accuracy: 0.4208 Validation Accuracy: 0.4174\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 1.565244 Training Accuracy: 0.4307 Validation Accuracy: 0.4220\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 1.529393 Training Accuracy: 0.4592 Validation Accuracy: 0.4262\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 1.529694 Training Accuracy: 0.4641 Validation Accuracy: 0.4278\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 1.554669 Training Accuracy: 0.4901 Validation Accuracy: 0.4330\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.553299 Training Accuracy: 0.4455 Validation Accuracy: 0.4390\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 1.508910 Training Accuracy: 0.4666 Validation Accuracy: 0.4388\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 1.471816 Training Accuracy: 0.4938 Validation Accuracy: 0.4492\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 1.447402 Training Accuracy: 0.4814 Validation Accuracy: 0.4474\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 1.495968 Training Accuracy: 0.5099 Validation Accuracy: 0.4500\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.498724 Training Accuracy: 0.4728 Validation Accuracy: 0.4528\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 1.425855 Training Accuracy: 0.4938 Validation Accuracy: 0.4570\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 1.430329 Training Accuracy: 0.5050 Validation Accuracy: 0.4482\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 1.400934 Training Accuracy: 0.4963 Validation Accuracy: 0.4666\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 1.438124 Training Accuracy: 0.5347 Validation Accuracy: 0.4666\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.480639 Training Accuracy: 0.4876 Validation Accuracy: 0.4620\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 1.389351 Training Accuracy: 0.5012 Validation Accuracy: 0.4716\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 1.361674 Training Accuracy: 0.5025 Validation Accuracy: 0.4706\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 1.358605 Training Accuracy: 0.5210 Validation Accuracy: 0.4732\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 1.393683 Training Accuracy: 0.5520 Validation Accuracy: 0.4760\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.396962 Training Accuracy: 0.5161 Validation Accuracy: 0.4782\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 1.330358 Training Accuracy: 0.5285 Validation Accuracy: 0.4744\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 1.319508 Training Accuracy: 0.5186 Validation Accuracy: 0.4796\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 1.297173 Training Accuracy: 0.5371 Validation Accuracy: 0.4874\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 1.347034 Training Accuracy: 0.5545 Validation Accuracy: 0.4826\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.360888 Training Accuracy: 0.5285 Validation Accuracy: 0.4876\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 1.315488 Training Accuracy: 0.5408 Validation Accuracy: 0.4826\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 1.273035 Training Accuracy: 0.5359 Validation Accuracy: 0.4878\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 1.264104 Training Accuracy: 0.5594 Validation Accuracy: 0.4896\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 1.301354 Training Accuracy: 0.5780 Validation Accuracy: 0.4944\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.327398 Training Accuracy: 0.5347 Validation Accuracy: 0.4950\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 1.282329 Training Accuracy: 0.5520 Validation Accuracy: 0.4928\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 1.248186 Training Accuracy: 0.5644 Validation Accuracy: 0.4932\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 1.236293 Training Accuracy: 0.5705 Validation Accuracy: 0.4870\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 1.252488 Training Accuracy: 0.5879 Validation Accuracy: 0.4990\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.278755 Training Accuracy: 0.5619 Validation Accuracy: 0.5012\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 1.238703 Training Accuracy: 0.5903 Validation Accuracy: 0.4970\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 1.212848 Training Accuracy: 0.5619 Validation Accuracy: 0.5060\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 1.188506 Training Accuracy: 0.5842 Validation Accuracy: 0.5004\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 1.227350 Training Accuracy: 0.6015 Validation Accuracy: 0.5124\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.255651 Training Accuracy: 0.5718 Validation Accuracy: 0.5110\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 1.209433 Training Accuracy: 0.5854 Validation Accuracy: 0.4986\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 1.184454 Training Accuracy: 0.5767 Validation Accuracy: 0.5036\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 1.175184 Training Accuracy: 0.6052 Validation Accuracy: 0.4998\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 1.180921 Training Accuracy: 0.6176 Validation Accuracy: 0.5100\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.206740 Training Accuracy: 0.5842 Validation Accuracy: 0.5032\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 1.178533 Training Accuracy: 0.6002 Validation Accuracy: 0.5064\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 1.141453 Training Accuracy: 0.6002 Validation Accuracy: 0.5122\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 1.120831 Training Accuracy: 0.6114 Validation Accuracy: 0.5154\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 1.162947 Training Accuracy: 0.6262 Validation Accuracy: 0.5144\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.177670 Training Accuracy: 0.6126 Validation Accuracy: 0.5106\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 1.143583 Training Accuracy: 0.6238 Validation Accuracy: 0.5136\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 1.113662 Training Accuracy: 0.6089 Validation Accuracy: 0.5192\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 1.103787 Training Accuracy: 0.6200 Validation Accuracy: 0.5030\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 1.122746 Training Accuracy: 0.6374 Validation Accuracy: 0.5226\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.164381 Training Accuracy: 0.6238 Validation Accuracy: 0.5154\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 1.111886 Training Accuracy: 0.6262 Validation Accuracy: 0.5228\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 1.066828 Training Accuracy: 0.6225 Validation Accuracy: 0.5202\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 1.055715 Training Accuracy: 0.6460 Validation Accuracy: 0.5206\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 1.094708 Training Accuracy: 0.6485 Validation Accuracy: 0.5274\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.116091 Training Accuracy: 0.6250 Validation Accuracy: 0.5242\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 1.082202 Training Accuracy: 0.6386 Validation Accuracy: 0.5200\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 1.073342 Training Accuracy: 0.6139 Validation Accuracy: 0.5298\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 1.034421 Training Accuracy: 0.6535 Validation Accuracy: 0.5286\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 1.057855 Training Accuracy: 0.6473 Validation Accuracy: 0.5294\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.118533 Training Accuracy: 0.6300 Validation Accuracy: 0.5198\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 1.074038 Training Accuracy: 0.6423 Validation Accuracy: 0.5300\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 1.032530 Training Accuracy: 0.6386 Validation Accuracy: 0.5264\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 1.030979 Training Accuracy: 0.6572 Validation Accuracy: 0.5212\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 1.026403 Training Accuracy: 0.6720 Validation Accuracy: 0.5352\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.066715 Training Accuracy: 0.6436 Validation Accuracy: 0.5320\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 1.037453 Training Accuracy: 0.6671 Validation Accuracy: 0.5330\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 0.992060 Training Accuracy: 0.6436 Validation Accuracy: 0.5354\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 0.992481 Training Accuracy: 0.6807 Validation Accuracy: 0.5284\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 0.998348 Training Accuracy: 0.6918 Validation Accuracy: 0.5428\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.050648 Training Accuracy: 0.6349 Validation Accuracy: 0.5324\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 1.032765 Training Accuracy: 0.6782 Validation Accuracy: 0.5336\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 0.981132 Training Accuracy: 0.6683 Validation Accuracy: 0.5390\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 0.954396 Training Accuracy: 0.6671 Validation Accuracy: 0.5406\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 0.974048 Training Accuracy: 0.6968 Validation Accuracy: 0.5334\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.019918 Training Accuracy: 0.6671 Validation Accuracy: 0.5354\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 0.979614 Training Accuracy: 0.6832 Validation Accuracy: 0.5374\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 0.947586 Training Accuracy: 0.6782 Validation Accuracy: 0.5412\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 0.911028 Training Accuracy: 0.6980 Validation Accuracy: 0.5410\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 0.934595 Training Accuracy: 0.6931 Validation Accuracy: 0.5452\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.983006 Training Accuracy: 0.6807 Validation Accuracy: 0.5444\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 0.950321 Training Accuracy: 0.6993 Validation Accuracy: 0.5470\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 0.917889 Training Accuracy: 0.6894 Validation Accuracy: 0.5444\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 0.900499 Training Accuracy: 0.7141 Validation Accuracy: 0.5504\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 0.911687 Training Accuracy: 0.6955 Validation Accuracy: 0.5500\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.955828 Training Accuracy: 0.6819 Validation Accuracy: 0.5478\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 0.942079 Training Accuracy: 0.6931 Validation Accuracy: 0.5462\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 0.890828 Training Accuracy: 0.7030 Validation Accuracy: 0.5468\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 0.872726 Training Accuracy: 0.7166 Validation Accuracy: 0.5476\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 0.901815 Training Accuracy: 0.7265 Validation Accuracy: 0.5524\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.924115 Training Accuracy: 0.7079 Validation Accuracy: 0.5454\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 0.931060 Training Accuracy: 0.7042 Validation Accuracy: 0.5468\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 0.893453 Training Accuracy: 0.7092 Validation Accuracy: 0.5482\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 0.861800 Training Accuracy: 0.7141 Validation Accuracy: 0.5486\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 0.869282 Training Accuracy: 0.7191 Validation Accuracy: 0.5496\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.913206 Training Accuracy: 0.7116 Validation Accuracy: 0.5548\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 0.911996 Training Accuracy: 0.7215 Validation Accuracy: 0.5488\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 0.859174 Training Accuracy: 0.7079 Validation Accuracy: 0.5530\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 0.824624 Training Accuracy: 0.7265 Validation Accuracy: 0.5544\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 0.843047 Training Accuracy: 0.7339 Validation Accuracy: 0.5558\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.889310 Training Accuracy: 0.7252 Validation Accuracy: 0.5564\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 0.868690 Training Accuracy: 0.7401 Validation Accuracy: 0.5552\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 0.829801 Training Accuracy: 0.7364 Validation Accuracy: 0.5610\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 0.811823 Training Accuracy: 0.7228 Validation Accuracy: 0.5606\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 0.805802 Training Accuracy: 0.7351 Validation Accuracy: 0.5558\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.867006 Training Accuracy: 0.7351 Validation Accuracy: 0.5606\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 0.855493 Training Accuracy: 0.7277 Validation Accuracy: 0.5562\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 0.801831 Training Accuracy: 0.7314 Validation Accuracy: 0.5624\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 0.781471 Training Accuracy: 0.7525 Validation Accuracy: 0.5602\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 0.792475 Training Accuracy: 0.7500 Validation Accuracy: 0.5608\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.842375 Training Accuracy: 0.7389 Validation Accuracy: 0.5602\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 0.832316 Training Accuracy: 0.7450 Validation Accuracy: 0.5550\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 0.784177 Training Accuracy: 0.7475 Validation Accuracy: 0.5680\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 0.772025 Training Accuracy: 0.7537 Validation Accuracy: 0.5670\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 0.766000 Training Accuracy: 0.7599 Validation Accuracy: 0.5656\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.820634 Training Accuracy: 0.7450 Validation Accuracy: 0.5580\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 0.832229 Training Accuracy: 0.7500 Validation Accuracy: 0.5608\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 0.780304 Training Accuracy: 0.7426 Validation Accuracy: 0.5636\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 0.746697 Training Accuracy: 0.7661 Validation Accuracy: 0.5632\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 0.758033 Training Accuracy: 0.7686 Validation Accuracy: 0.5648\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.815235 Training Accuracy: 0.7525 Validation Accuracy: 0.5728\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 0.777961 Training Accuracy: 0.7686 Validation Accuracy: 0.5644\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 0.760212 Training Accuracy: 0.7723 Validation Accuracy: 0.5702\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 0.719086 Training Accuracy: 0.7710 Validation Accuracy: 0.5682\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 0.729682 Training Accuracy: 0.7698 Validation Accuracy: 0.5648\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.794072 Training Accuracy: 0.7636 Validation Accuracy: 0.5710\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 0.768721 Training Accuracy: 0.7475 Validation Accuracy: 0.5670\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 0.726832 Training Accuracy: 0.7649 Validation Accuracy: 0.5656\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 0.720519 Training Accuracy: 0.7859 Validation Accuracy: 0.5714\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 0.714642 Training Accuracy: 0.7785 Validation Accuracy: 0.5690\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.755400 Training Accuracy: 0.7673 Validation Accuracy: 0.5728\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 0.743835 Training Accuracy: 0.7686 Validation Accuracy: 0.5738\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 0.707242 Training Accuracy: 0.7748 Validation Accuracy: 0.5702\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 0.684332 Training Accuracy: 0.8007 Validation Accuracy: 0.5712\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 0.698892 Training Accuracy: 0.7884 Validation Accuracy: 0.5716\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.739672 Training Accuracy: 0.7871 Validation Accuracy: 0.5752\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 0.723055 Training Accuracy: 0.7797 Validation Accuracy: 0.5696\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 0.694817 Training Accuracy: 0.7809 Validation Accuracy: 0.5736\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 0.685956 Training Accuracy: 0.7921 Validation Accuracy: 0.5748\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 0.678009 Training Accuracy: 0.7933 Validation Accuracy: 0.5706\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.734871 Training Accuracy: 0.7797 Validation Accuracy: 0.5766\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 0.718360 Training Accuracy: 0.7908 Validation Accuracy: 0.5700\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 0.683446 Training Accuracy: 0.7834 Validation Accuracy: 0.5690\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 0.665730 Training Accuracy: 0.7970 Validation Accuracy: 0.5746\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 0.666854 Training Accuracy: 0.7970 Validation Accuracy: 0.5668\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.712063 Training Accuracy: 0.7983 Validation Accuracy: 0.5720\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 0.714050 Training Accuracy: 0.7946 Validation Accuracy: 0.5736\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 0.660751 Training Accuracy: 0.7946 Validation Accuracy: 0.5722\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 0.636276 Training Accuracy: 0.8119 Validation Accuracy: 0.5714\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 0.683342 Training Accuracy: 0.8020 Validation Accuracy: 0.5710\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.700179 Training Accuracy: 0.7896 Validation Accuracy: 0.5730\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 0.706330 Training Accuracy: 0.7772 Validation Accuracy: 0.5750\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 0.660846 Training Accuracy: 0.7958 Validation Accuracy: 0.5732\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 0.628210 Training Accuracy: 0.8181 Validation Accuracy: 0.5788\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 0.656037 Training Accuracy: 0.8106 Validation Accuracy: 0.5822\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.673499 Training Accuracy: 0.8082 Validation Accuracy: 0.5772\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 0.665899 Training Accuracy: 0.8131 Validation Accuracy: 0.5810\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 0.648377 Training Accuracy: 0.7995 Validation Accuracy: 0.5780\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 0.612403 Training Accuracy: 0.8156 Validation Accuracy: 0.5804\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 0.636995 Training Accuracy: 0.8193 Validation Accuracy: 0.5762\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.679455 Training Accuracy: 0.7908 Validation Accuracy: 0.5768\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 0.652624 Training Accuracy: 0.8057 Validation Accuracy: 0.5870\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 0.622411 Training Accuracy: 0.8007 Validation Accuracy: 0.5788\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 0.580404 Training Accuracy: 0.8354 Validation Accuracy: 0.5886\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 0.622835 Training Accuracy: 0.8243 Validation Accuracy: 0.5758\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.659122 Training Accuracy: 0.8007 Validation Accuracy: 0.5740\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 0.646624 Training Accuracy: 0.8243 Validation Accuracy: 0.5750\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 0.629793 Training Accuracy: 0.8069 Validation Accuracy: 0.5820\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 0.587480 Training Accuracy: 0.8280 Validation Accuracy: 0.5830\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 0.610497 Training Accuracy: 0.8205 Validation Accuracy: 0.5736\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.650852 Training Accuracy: 0.8193 Validation Accuracy: 0.5808\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 0.649979 Training Accuracy: 0.8243 Validation Accuracy: 0.5764\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 0.614022 Training Accuracy: 0.8106 Validation Accuracy: 0.5810\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 0.577361 Training Accuracy: 0.8465 Validation Accuracy: 0.5884\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 0.600209 Training Accuracy: 0.8354 Validation Accuracy: 0.5742\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.643454 Training Accuracy: 0.8243 Validation Accuracy: 0.5802\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 0.628937 Training Accuracy: 0.8243 Validation Accuracy: 0.5762\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 0.606527 Training Accuracy: 0.8094 Validation Accuracy: 0.5812\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 0.575835 Training Accuracy: 0.8428 Validation Accuracy: 0.5858\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 0.571915 Training Accuracy: 0.8329 Validation Accuracy: 0.5826\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.626236 Training Accuracy: 0.8329 Validation Accuracy: 0.5838\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 0.598653 Training Accuracy: 0.8354 Validation Accuracy: 0.5856\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 0.587948 Training Accuracy: 0.8255 Validation Accuracy: 0.5796\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 0.545377 Training Accuracy: 0.8589 Validation Accuracy: 0.5864\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 0.552128 Training Accuracy: 0.8577 Validation Accuracy: 0.5830\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.603486 Training Accuracy: 0.8366 Validation Accuracy: 0.5900\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 0.587996 Training Accuracy: 0.8428 Validation Accuracy: 0.5800\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 0.561316 Training Accuracy: 0.8391 Validation Accuracy: 0.5876\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 0.539519 Training Accuracy: 0.8478 Validation Accuracy: 0.5890\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 0.524285 Training Accuracy: 0.8626 Validation Accuracy: 0.5890\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.580976 Training Accuracy: 0.8614 Validation Accuracy: 0.5902\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 0.573119 Training Accuracy: 0.8502 Validation Accuracy: 0.5846\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 0.543794 Training Accuracy: 0.8577 Validation Accuracy: 0.5910\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 0.516992 Training Accuracy: 0.8676 Validation Accuracy: 0.5948\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 0.529623 Training Accuracy: 0.8663 Validation Accuracy: 0.5866\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.559510 Training Accuracy: 0.8639 Validation Accuracy: 0.5882\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 0.552689 Training Accuracy: 0.8639 Validation Accuracy: 0.5920\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 0.538077 Training Accuracy: 0.8515 Validation Accuracy: 0.5878\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 0.508676 Training Accuracy: 0.8614 Validation Accuracy: 0.5926\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 0.520105 Training Accuracy: 0.8601 Validation Accuracy: 0.5912\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.557352 Training Accuracy: 0.8577 Validation Accuracy: 0.5912\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 0.554148 Training Accuracy: 0.8688 Validation Accuracy: 0.5840\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 0.520423 Training Accuracy: 0.8577 Validation Accuracy: 0.5940\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 0.506834 Training Accuracy: 0.8651 Validation Accuracy: 0.5914\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 0.523919 Training Accuracy: 0.8651 Validation Accuracy: 0.5884\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.572298 Training Accuracy: 0.8453 Validation Accuracy: 0.5864\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 0.544840 Training Accuracy: 0.8527 Validation Accuracy: 0.5872\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 0.520403 Training Accuracy: 0.8639 Validation Accuracy: 0.5964\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 0.490850 Training Accuracy: 0.8824 Validation Accuracy: 0.5932\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 0.523943 Training Accuracy: 0.8527 Validation Accuracy: 0.5954\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.537330 Training Accuracy: 0.8713 Validation Accuracy: 0.5964\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 0.517348 Training Accuracy: 0.8688 Validation Accuracy: 0.5956\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 0.500541 Training Accuracy: 0.8651 Validation Accuracy: 0.5970\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 0.480249 Training Accuracy: 0.8762 Validation Accuracy: 0.5918\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 0.492932 Training Accuracy: 0.8762 Validation Accuracy: 0.5966\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.526611 Training Accuracy: 0.8762 Validation Accuracy: 0.5944\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 0.509374 Training Accuracy: 0.8800 Validation Accuracy: 0.5908\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 0.505521 Training Accuracy: 0.8577 Validation Accuracy: 0.5906\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 0.466427 Training Accuracy: 0.8762 Validation Accuracy: 0.5978\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 0.465280 Training Accuracy: 0.8911 Validation Accuracy: 0.5972\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.502774 Training Accuracy: 0.8775 Validation Accuracy: 0.5936\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 0.474586 Training Accuracy: 0.8824 Validation Accuracy: 0.6010\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 0.488048 Training Accuracy: 0.8626 Validation Accuracy: 0.5922\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 0.444122 Training Accuracy: 0.9010 Validation Accuracy: 0.5970\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 0.461085 Training Accuracy: 0.8837 Validation Accuracy: 0.5918\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.488097 Training Accuracy: 0.8800 Validation Accuracy: 0.6018\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 0.462922 Training Accuracy: 0.8948 Validation Accuracy: 0.5984\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 0.471163 Training Accuracy: 0.8700 Validation Accuracy: 0.5962\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 0.425770 Training Accuracy: 0.9047 Validation Accuracy: 0.5996\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 0.455101 Training Accuracy: 0.8899 Validation Accuracy: 0.5966\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.485024 Training Accuracy: 0.8911 Validation Accuracy: 0.6004\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 0.456982 Training Accuracy: 0.8960 Validation Accuracy: 0.5988\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 0.458976 Training Accuracy: 0.8725 Validation Accuracy: 0.5972\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 0.420433 Training Accuracy: 0.8985 Validation Accuracy: 0.5968\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 0.429962 Training Accuracy: 0.8923 Validation Accuracy: 0.6060\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.476300 Training Accuracy: 0.8899 Validation Accuracy: 0.5996\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 0.442268 Training Accuracy: 0.9035 Validation Accuracy: 0.5972\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 0.439630 Training Accuracy: 0.8812 Validation Accuracy: 0.6014\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 0.434827 Training Accuracy: 0.8886 Validation Accuracy: 0.5958\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 0.443855 Training Accuracy: 0.8923 Validation Accuracy: 0.5924\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.471676 Training Accuracy: 0.8738 Validation Accuracy: 0.5974\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 0.443988 Training Accuracy: 0.8899 Validation Accuracy: 0.6002\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 0.433045 Training Accuracy: 0.8899 Validation Accuracy: 0.5978\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 0.435553 Training Accuracy: 0.8973 Validation Accuracy: 0.6056\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 0.425166 Training Accuracy: 0.9084 Validation Accuracy: 0.6056\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.470638 Training Accuracy: 0.8787 Validation Accuracy: 0.6052\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 0.431677 Training Accuracy: 0.9084 Validation Accuracy: 0.6000\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 0.424855 Training Accuracy: 0.8824 Validation Accuracy: 0.5916\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 0.400300 Training Accuracy: 0.9097 Validation Accuracy: 0.5992\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 0.419189 Training Accuracy: 0.9072 Validation Accuracy: 0.5970\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.462130 Training Accuracy: 0.8911 Validation Accuracy: 0.5976\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 0.422640 Training Accuracy: 0.9035 Validation Accuracy: 0.6008\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 0.409152 Training Accuracy: 0.8911 Validation Accuracy: 0.5972\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 0.386824 Training Accuracy: 0.9109 Validation Accuracy: 0.5958\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 0.410939 Training Accuracy: 0.9146 Validation Accuracy: 0.5998\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.445714 Training Accuracy: 0.8998 Validation Accuracy: 0.6080\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 0.415983 Training Accuracy: 0.9022 Validation Accuracy: 0.5936\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 0.411971 Training Accuracy: 0.8837 Validation Accuracy: 0.6004\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 0.390390 Training Accuracy: 0.9171 Validation Accuracy: 0.6040\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 0.399900 Training Accuracy: 0.9109 Validation Accuracy: 0.6040\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.441310 Training Accuracy: 0.8861 Validation Accuracy: 0.5948\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 0.415945 Training Accuracy: 0.9059 Validation Accuracy: 0.5954\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 0.410923 Training Accuracy: 0.9010 Validation Accuracy: 0.5932\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 0.374693 Training Accuracy: 0.9109 Validation Accuracy: 0.6058\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 0.414483 Training Accuracy: 0.8985 Validation Accuracy: 0.5942\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.453534 Training Accuracy: 0.8886 Validation Accuracy: 0.6008\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 0.397902 Training Accuracy: 0.9171 Validation Accuracy: 0.5984\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 0.404451 Training Accuracy: 0.8886 Validation Accuracy: 0.5894\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 0.382336 Training Accuracy: 0.9134 Validation Accuracy: 0.6082\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 0.395423 Training Accuracy: 0.9158 Validation Accuracy: 0.6060\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.414898 Training Accuracy: 0.9035 Validation Accuracy: 0.6056\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 0.399257 Training Accuracy: 0.9171 Validation Accuracy: 0.5942\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 0.380300 Training Accuracy: 0.9072 Validation Accuracy: 0.6038\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 0.368914 Training Accuracy: 0.9035 Validation Accuracy: 0.6084\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 0.372647 Training Accuracy: 0.9233 Validation Accuracy: 0.6060\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.406568 Training Accuracy: 0.9183 Validation Accuracy: 0.5966\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 0.381093 Training Accuracy: 0.9158 Validation Accuracy: 0.6026\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 0.384395 Training Accuracy: 0.9010 Validation Accuracy: 0.6002\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 0.345229 Training Accuracy: 0.9282 Validation Accuracy: 0.6074\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 0.367679 Training Accuracy: 0.9072 Validation Accuracy: 0.6004\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.408083 Training Accuracy: 0.9109 Validation Accuracy: 0.6038\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 0.368405 Training Accuracy: 0.9233 Validation Accuracy: 0.6080\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 0.367147 Training Accuracy: 0.9158 Validation Accuracy: 0.6022\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 0.355301 Training Accuracy: 0.9158 Validation Accuracy: 0.6102\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 0.359661 Training Accuracy: 0.9319 Validation Accuracy: 0.6108\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.402830 Training Accuracy: 0.9097 Validation Accuracy: 0.5960\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 0.376934 Training Accuracy: 0.9171 Validation Accuracy: 0.6022\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 0.356744 Training Accuracy: 0.9121 Validation Accuracy: 0.6058\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 0.340836 Training Accuracy: 0.9282 Validation Accuracy: 0.6100\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 0.362867 Training Accuracy: 0.9307 Validation Accuracy: 0.6068\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.385688 Training Accuracy: 0.9146 Validation Accuracy: 0.6046\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 0.350754 Training Accuracy: 0.9257 Validation Accuracy: 0.6074\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 0.336604 Training Accuracy: 0.9233 Validation Accuracy: 0.6052\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 0.334061 Training Accuracy: 0.9245 Validation Accuracy: 0.6100\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 0.328359 Training Accuracy: 0.9295 Validation Accuracy: 0.6114\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.390184 Training Accuracy: 0.9208 Validation Accuracy: 0.6058\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 0.348089 Training Accuracy: 0.9332 Validation Accuracy: 0.6010\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 0.352769 Training Accuracy: 0.9121 Validation Accuracy: 0.6116\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 0.327993 Training Accuracy: 0.9319 Validation Accuracy: 0.6106\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 0.335090 Training Accuracy: 0.9356 Validation Accuracy: 0.6098\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.378274 Training Accuracy: 0.9257 Validation Accuracy: 0.6032\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 0.345503 Training Accuracy: 0.9356 Validation Accuracy: 0.6028\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 0.325114 Training Accuracy: 0.9245 Validation Accuracy: 0.6098\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 0.329407 Training Accuracy: 0.9319 Validation Accuracy: 0.6104\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 0.325473 Training Accuracy: 0.9369 Validation Accuracy: 0.6070\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.352299 Training Accuracy: 0.9171 Validation Accuracy: 0.6102\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 0.331963 Training Accuracy: 0.9369 Validation Accuracy: 0.6146\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 0.320457 Training Accuracy: 0.9307 Validation Accuracy: 0.6054\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 0.306790 Training Accuracy: 0.9381 Validation Accuracy: 0.6092\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 0.312991 Training Accuracy: 0.9431 Validation Accuracy: 0.6114\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6081355810165405\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEv1/6T+DxtR6uEAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f64dbc7e1d0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. That's because there are many more techniques that can be applied to your model and we recemmond that once you are done with this project, you explore!\n",
    "\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
